{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# I’m Something of a Painter Myself\n",
    "\n",
    "### Use GANs to create art - will you be the next Monet?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60b1a09f7ae0ea4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Description\n",
    "\n",
    "*“Every artist dips his brush in his own soul, and paints his own nature into his pictures.”*\n",
    "\n",
    "-Henry Ward Beecher\n",
    "\n",
    "We recognize the works of artists through their unique style, such as color choices or brush strokes. The “je ne sais quoi” of artists like Claude Monet can now be imitated with algorithms thanks to generative adversarial networks (GANs). In this getting started competition, you will bring that style to your photos or recreate the style from scratch!\n",
    "\n",
    "Computer vision has advanced tremendously in recent years and GANs are now capable of mimicking objects in a very convincing way. But creating museum-worthy masterpieces is thought of to be, well, more art than science. So can (data) science, in the form of GANs, trick classifiers into believing you’ve created a true Monet? That’s the challenge you’ll take on!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b7cb98b8f7ce90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Challenge:\n",
    "\n",
    "A GAN consists of at least two neural networks: a generator model and a discriminator model. The generator is a neural network that creates the images. For our competition, you should generate images in the style of Monet. This generator is trained using a discriminator.\n",
    "\n",
    "The two models will work against each other, with the generator trying to trick the discriminator, and the discriminator trying to accurately classify the real vs. generated images.\n",
    "\n",
    "Your task is to build a GAN that generates 7,000 to 10,000 Monet-style images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9cd3e6f84d21677"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset Description\n",
    "\n",
    "The dataset contains four directories: monet_tfrec, photo_tfrec, monet_jpg, and photo_jpg. The monet_tfrec and monet_jpg directories contain the same painting images, and the photo_tfrec and photo_jpg directories contain the same photos.\n",
    "\n",
    "We recommend using TFRecords as a Getting Started competition is a great way to become more familiar with a new data format, but JPEG images have also been provided.\n",
    "\n",
    "The monet directories contain Monet paintings. Use these images to train your model.\n",
    "\n",
    "The photo directories contain photos. Add Monet-style to these images and submit your generated jpeg images as a zip file. Other photos outside of this dataset can be transformed but keep your submission file limited to 10,000 images.\n",
    "\n",
    "Note: Monet-style art can be created from scratch using other GAN architectures like DCGAN. The submitted image files do not necessarily have to be transformed photos.\n",
    "\n",
    "Check out the CycleGAN dataset to experiment with the artistic style of other artists."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b233f837c1312504"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Files\n",
    "\n",
    "monet_jpg - 300 Monet paintings sized 256x256 in JPEG format\n",
    "\n",
    "monet_tfrec - 300 Monet paintings sized 256x256 in TFRecord format\n",
    "\n",
    "photo_jpg - 7028 photos sized 256x256 in JPEG format\n",
    "\n",
    "photo_tfrec - 7028 photos sized 256x256 in TFRecord format\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a40eb8fdfdfa9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from math import *\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-28T00:55:21.474385Z"
    }
   },
   "id": "57db32f6a74362e1"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "files_monet_jpg = os.listdir(\"photos/monet_jpg\")\n",
    "files_photo_jpg = os.listdir(\"photos/photo_jpg\")\n",
    "files_monet_jpg_path = []\n",
    "files_photo_jpg_path = []\n",
    "for i in files_monet_jpg:\n",
    "    files_monet_jpg_path.append(\"photos/monet_jpg/\"+i)\n",
    "for i in files_photo_jpg:\n",
    "    files_photo_jpg_path.append(\"photos/photo_jpg/\"+i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:31:51.781948Z",
     "start_time": "2023-08-28T00:31:51.751547Z"
    }
   },
   "id": "e9f56988aeda363a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "X_train_size = int(len(files_photo_jpg_path)*0.70)\n",
    "X_valid_size = int(len(files_photo_jpg_path)*0.15)\n",
    "X_test_size = int(len(files_photo_jpg_path)*0.15)\n",
    "\n",
    "y_train_size = int(len(files_monet_jpg_path)*0.70)\n",
    "y_valid_size = int(len(files_monet_jpg_path)*0.15)\n",
    "y_test_size = int(len(files_monet_jpg_path)*0.15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:31:52.153087Z",
     "start_time": "2023-08-28T00:31:52.150333Z"
    }
   },
   "id": "542b3aafbd4cd5c9"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "train_photo_jpg = files_photo_jpg_path[0:X_train_size]\n",
    "valid_photo_jpg = files_photo_jpg_path[X_train_size:X_train_size+X_valid_size]\n",
    "test_photo_jpg = files_photo_jpg_path[X_train_size+X_valid_size:]\n",
    "\n",
    "train_monet_jpg = files_monet_jpg_path[0:y_train_size]\n",
    "valid_monet_jpg = files_monet_jpg_path[y_train_size:y_train_size+y_valid_size]\n",
    "test_monet_jpg = files_monet_jpg_path[y_train_size+y_valid_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:31:52.531138Z",
     "start_time": "2023-08-28T00:31:52.529237Z"
    }
   },
   "id": "d2a6d4c32913c07e"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "img = tf.keras.preprocessing.image.load_img(path=\"photos/monet_jpg/0a5075d42a.jpg\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:42:22.467637Z",
     "start_time": "2023-08-28T00:42:22.462847Z"
    }
   },
   "id": "b3946166555866fe"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "tens = tf.convert_to_tensor(img)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:42:22.903225Z",
     "start_time": "2023-08-28T00:42:22.899605Z"
    }
   },
   "id": "a7b26192416c7f19"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(4926, 256, 256, 3)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = []\n",
    "X_valid = []\n",
    "X_test = []\n",
    "\n",
    "for i in train_photo_jpg:\n",
    "    img = tf.keras.preprocessing.image.load_img(path=i)\n",
    "    tensor = tf.convert_to_tensor(img)\n",
    "    X_train.append(tensor)\n",
    "\n",
    "for i in valid_photo_jpg:\n",
    "    img = tf.keras.preprocessing.image.load_img(path=i)\n",
    "    tensor = tf.convert_to_tensor(img)\n",
    "    X_valid.append(tensor)\n",
    "\n",
    "for i in test_photo_jpg:\n",
    "    img = tf.keras.preprocessing.image.load_img(path=i)\n",
    "    tensor = tf.convert_to_tensor(img)\n",
    "    X_test.append(tensor)\n",
    "\n",
    "X_train = np.array(X_train)/255.0\n",
    "X_valid = np.array(X_valid)/255.0\n",
    "X_test = np.array(X_test)/255.0\n",
    "\n",
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:35:32.693548Z",
     "start_time": "2023-08-28T00:34:59.920160Z"
    }
   },
   "id": "a69d14c58404abc1"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(210, 256, 256, 3)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = []\n",
    "y_valid = []\n",
    "y_test = []\n",
    "\n",
    "for i in train_monet_jpg:\n",
    "    img = tf.keras.preprocessing.image.load_img(path=i)\n",
    "    tensor = tf.convert_to_tensor(img)\n",
    "    y_train.append(tensor)\n",
    "\n",
    "for i in valid_monet_jpg:\n",
    "    img = tf.keras.preprocessing.image.load_img(path=i)\n",
    "    tensor = tf.convert_to_tensor(img)\n",
    "    y_valid.append(tensor)\n",
    "\n",
    "for i in test_monet_jpg:\n",
    "    img = tf.keras.preprocessing.image.load_img(path=i)\n",
    "    tensor = tf.convert_to_tensor(img)\n",
    "    y_test.append(tensor)\n",
    "\n",
    "y_train = np.array(y_train)/255.0\n",
    "y_valid = np.array(y_valid)/255.0\n",
    "y_test = np.array(y_test)/255.0\n",
    "\n",
    "y_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:35:33.394305Z",
     "start_time": "2023-08-28T00:35:32.713302Z"
    }
   },
   "id": "8d8da04c7bc31459"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor = tf.convert_to_tensor(img)\n",
    "img_tensor.ndim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:35:36.832430Z",
     "start_time": "2023-08-28T00:35:36.822822Z"
    }
   },
   "id": "152a3192556e2f83"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\",input_shape=(256,256,3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=32,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=8,activation=\"softmax\"),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:35:37.567236Z",
     "start_time": "2023-08-28T00:35:37.400048Z"
    }
   },
   "id": "aa9026ec3894630e"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 254, 254, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 127, 127, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 125, 125, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 62, 62, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 60, 60, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 30, 30, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 128)       295040    \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 14, 14, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 12, 12, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                73760     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 813672 (3.10 MB)\n",
      "Trainable params: 813672 (3.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:34:41.553527Z",
     "start_time": "2023-08-28T00:34:41.537626Z"
    }
   },
   "id": "4a19589236ea32dd"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 4926\n  y sizes: 210\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m\"\u001B[39m,loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m,metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m----> 2\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train,y_train,batch_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m4\u001B[39m,\u001B[38;5;241m4\u001B[39m),epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,validation_data\u001B[38;5;241m=\u001B[39m(X_valid,y_valid))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1950\u001B[0m, in \u001B[0;36m_check_data_cardinality\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m   1943\u001B[0m     msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m sizes: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1944\u001B[0m         label,\n\u001B[1;32m   1945\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m   1946\u001B[0m             \u001B[38;5;28mstr\u001B[39m(i\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mflatten(single_data)\n\u001B[1;32m   1947\u001B[0m         ),\n\u001B[1;32m   1948\u001B[0m     )\n\u001B[1;32m   1949\u001B[0m msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMake sure all arrays contain the same number of samples.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1950\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[0;31mValueError\u001B[0m: Data cardinality is ambiguous:\n  x sizes: 4926\n  y sizes: 210\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit(X_train,y_train,batch_size=(4,4),epochs=10,validation_data=(X_valid,y_valid))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:39:42.298881Z",
     "start_time": "2023-08-28T00:37:09.121172Z"
    }
   },
   "id": "73ae1ab3c8f97fde"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files_photo_jpg_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m X_train \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m files_photo_jpg_path:\n\u001B[1;32m      3\u001B[0m     img \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mpreprocessing\u001B[38;5;241m.\u001B[39mimage\u001B[38;5;241m.\u001B[39mload_img(path\u001B[38;5;241m=\u001B[39mi)\n\u001B[1;32m      4\u001B[0m     tensor \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(img)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'files_photo_jpg_path' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for i in files_photo_jpg_path:\n",
    "    img = tf.keras.preprocessing.image.load_img(path=i)\n",
    "    tensor = tf.convert_to_tensor(img)\n",
    "    X_train.append(tensor)\n",
    "X_train = np.array(X_train)/255.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:55:15.053330Z",
     "start_time": "2023-08-28T00:55:15.041712Z"
    }
   },
   "id": "e73de030e20849b6"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in files_monet_jpg_path:\n",
    "    img = tf.keras.preprocessing.image.load_img(path=i)\n",
    "    tensor = tf.convert_to_tensor(img)\n",
    "    y_train.append(tensor)\n",
    "y_train = np.array(y_train)/255.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:48:28.870329Z",
     "start_time": "2023-08-28T00:48:28.407647Z"
    }
   },
   "id": "d23226a63a4234bc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m X_train[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T00:55:12.188643Z",
     "start_time": "2023-08-28T00:55:11.944933Z"
    }
   },
   "id": "1b13add230b8f62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b829e1d52c07d169"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
