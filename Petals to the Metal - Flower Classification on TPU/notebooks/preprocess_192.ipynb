{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def conv_to_char(n):\n",
    "    s = np.array(list(n))\n",
    "    st = \"\"\n",
    "    for i in s:\n",
    "        st += chr(i)\n",
    "    return st"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T14:53:21.153226Z",
     "start_time": "2023-09-25T14:53:21.151078Z"
    }
   },
   "id": "9ca83876c541b81f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def features_of_images_train_val(location_folder):\n",
    "\n",
    "    list_of_files = os.listdir(location_folder)\n",
    "\n",
    "    full_file_path = np.array([])\n",
    "    for i in list_of_files:\n",
    "        full_file_path = np.append(full_file_path,location_folder+i)\n",
    "\n",
    "    id = np.array([])\n",
    "    cls = np.array([])\n",
    "    image = np.array([])\n",
    "    ctr = 0\n",
    "    for i in full_file_path:\n",
    "        raw_img = tf.data.TFRecordDataset(i)\n",
    "        image_description = {\n",
    "            \"id\" : tf.io.FixedLenFeature([],tf.string),\n",
    "            \"class\": tf.io.FixedLenFeature([],tf.int64),\n",
    "            \"image\": tf.io.FixedLenFeature([],tf.string)\n",
    "        }\n",
    "        y = lambda x: tf.io.parse_single_example(x,image_description)\n",
    "        img = raw_img.map(y)\n",
    "\n",
    "        begin = i.rindex(\"-\")+1\n",
    "        end = i.index(\".tfrec\")\n",
    "        num = int(i[begin:end])\n",
    "\n",
    "        for dr in img.take(num):\n",
    "            id = np.append(id,conv_to_char(dr[\"id\"].numpy()))\n",
    "            cls = np.append(cls,int(dr[\"class\"].numpy()))\n",
    "            image = np.append(image,tf.io.decode_image(dr[\"image\"].numpy(),dtype=\"float64\"))\n",
    "            print(ctr)\n",
    "            ctr += 1\n",
    "    return id,cls,image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc06b1831aa9de14"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def features_of_images_test(location_folder):\n",
    "    list_of_files = os.listdir(location_folder)\n",
    "    \n",
    "    full_file_path = np.array([])\n",
    "    for i in list_of_files:\n",
    "        full_file_path = np.append(full_file_path,location_folder+i)\n",
    "\n",
    "    id = np.array([])\n",
    "    image = np.array([])\n",
    "\n",
    "    for i in full_file_path:\n",
    "        raw_img = tf.data.TFRecordDataset(i)\n",
    "        image_description = {\n",
    "            \"id\" : tf.io.FixedLenFeature([],tf.string),\n",
    "            \"class\": tf.io.FixedLenFeature([],tf.int64),\n",
    "            \"image\": tf.io.FixedLenFeature([],tf.string)\n",
    "        }\n",
    "        y = lambda x: tf.io.parse_single_example(x,image_description)\n",
    "        img = raw_img.map(y)\n",
    "\n",
    "        begin = i.rindex(\"-\")+1\n",
    "        end = i.index(\".tfrec\")\n",
    "        num = int(i[begin:end])\n",
    "\n",
    "        for dr in img.take(num):\n",
    "            id = np.append(id,conv_to_char(dr[\"id\"].numpy()))\n",
    "            image = np.append(image,tf.io.decode_image(dr[\"image\"].numpy(),dtype=\"float64\"))\n",
    "\n",
    "    return id,image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:09:09.109724Z",
     "start_time": "2023-09-25T15:09:09.101613Z"
    }
   },
   "id": "a3368db50e46ca11"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.]\n",
      "[4. 5.]\n"
     ]
    }
   ],
   "source": [
    "p = np.array([])\n",
    "p = np.append(p,4)\n",
    "print(p)\n",
    "p = np.append(p,5)\n",
    "print(p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:09:10.432420Z",
     "start_time": "2023-09-25T15:09:10.428443Z"
    }
   },
   "id": "a32808324ee79ed4"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "location_folder_train_192 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-192x192/train/\"\n",
    "\n",
    "location_folder_train_224 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-224x224/train/\"\n",
    "\n",
    "location_folder_train_331 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-331x331/train/\"\n",
    "\n",
    "location_folder_train_512 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-512x512/train/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:09:10.933230Z",
     "start_time": "2023-09-25T15:09:10.929446Z"
    }
   },
   "id": "9023735b9f7670c6"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "location_folder_val_192 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-192x192/val/\"\n",
    "\n",
    "location_folder_val_224 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-224x224/val/\"\n",
    "\n",
    "location_folder_val_331 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-331x331/val/\"\n",
    "\n",
    "location_folder_val_512 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-512x512/val/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:09:11.284814Z",
     "start_time": "2023-09-25T15:09:11.281684Z"
    }
   },
   "id": "f5e6cfc358f4a77"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "location_folder_test_192 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-192x192/test/\"\n",
    "\n",
    "location_folder_test_224 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-224x224/test/\"\n",
    "\n",
    "location_folder_test_331 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-331x331/test/\"\n",
    "\n",
    "location_folder_test_512 = \"/Users/amith/Documents/GitHub/kaggle/Petals to the Metal - Flower Classification on TPU/tpu-getting-started/tfrecords-jpeg-512x512/test/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:09:11.599078Z",
     "start_time": "2023-09-25T15:09:11.595917Z"
    }
   },
   "id": "39bffd3f10463c88"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2a0063f9f7b8c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_train_192,cls_train_192,image_train_192 = features_of_images_train_val(location_folder_train_192)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5405e3a5e75e20a3"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "id_train_224,cls_train_224,image_train_224 = features_of_images_train_val(location_folder_train_224)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:06:53.978739Z",
     "start_time": "2023-09-25T15:06:53.973763Z"
    }
   },
   "id": "acbd614186e8354"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "id_train_331,cls_train_331,image_train_331 = features_of_images_train_val(location_folder_train_331)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:06:59.027969Z",
     "start_time": "2023-09-25T15:06:59.023292Z"
    }
   },
   "id": "1716bca48957ae2c"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "id_train_512,cls_train_512,image_train_512 = features_of_images_train_val(location_folder_train_512)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:07:00.383389Z",
     "start_time": "2023-09-25T15:07:00.378307Z"
    }
   },
   "id": "478a2b939104191f"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_train_192)+len(id_train_224)+len(id_train_331)+len(id_train_512)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:07:01.298584Z",
     "start_time": "2023-09-25T15:07:01.282914Z"
    }
   },
   "id": "66263d7bec565309"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validation Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c77cd770ad4c213c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_val_192,cls_val_192,image_val_192 = features_of_images_train_val(location_folder_val_192)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2893568b5f0d1b19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_val_224,cls_val_224,image_val_224 = features_of_images_train_val(location_folder_val_224)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "eff51226d3f27372"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_val_331,cls_val_331,image_val_331 = features_of_images_train_val(location_folder_val_331)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3d56b309b82d5a32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_val_512,cls_val_512,image_val_512 = features_of_images_train_val(location_folder_val_512)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6e07ee7389e39101"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "253fb48bdc122624"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_test_192,image_test_192 = features_of_images_test(location_folder_test_192)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "efa4607c16654399"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_test_224,image_test_224 = features_of_images_test(location_folder_test_224)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e3fd18687173966c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_test_331,image_test_331 = features_of_images_test(location_folder_test_331)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2cb7fe228dd537db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_test_512,image_test_512 = features_of_images_test(location_folder_test_512)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3a3294508d3ce712"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deep Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c69cf6df65d90f8"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Resizing(height=512,width=512),\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=4,activation=\"relu\",input_shape=(512,512,3)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=4),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=128,kernel_size=8,activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=4),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=256,kernel_size=16,activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=4),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=128,kernel_size=8,activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=4),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=4,activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=4),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(units=128,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=104,activation=\"softmax\")\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T03:35:39.965698Z",
     "start_time": "2023-09-25T03:35:39.924776Z"
    }
   },
   "id": "86196da1bc54f7f1"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.RMSprop(),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=tf.keras.metrics.categorical_accuracy,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T03:35:40.525141Z",
     "start_time": "2023-09-25T03:35:40.515611Z"
    }
   },
   "id": "1f39fd4b33113975"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 354, in compute_output_shape\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_6' (type Sequential).\n    \n    One of the dimensions in the output is <= 0 due to downsampling in conv2d_3. Consider increasing the input size. Received input shape [None, 3, 3, 256] which would produce output shape with a zero or negative value in a dimension.\n    \n    Call arguments received by layer 'sequential_6' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 192, 192, 3), dtype=uint8)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_train_192\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcls_train_192\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEarlyStopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/var/folders/3l/ptr6dqdd4xl0jj13j9gj2y2r0000gn/T/__autograph_generated_file8paec43u.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 354, in compute_output_shape\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_6' (type Sequential).\n    \n    One of the dimensions in the output is <= 0 due to downsampling in conv2d_3. Consider increasing the input size. Received input shape [None, 3, 3, 256] which would produce output shape with a zero or negative value in a dimension.\n    \n    Call arguments received by layer 'sequential_6' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 192, 192, 3), dtype=uint8)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=image_train_192,\n",
    "    y=cls_train_192,\n",
    "    batch_size=10,\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(patience=2)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T03:35:44.381261Z",
     "start_time": "2023-09-25T03:35:41.228905Z"
    }
   },
   "id": "ac51ed570b2203b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f852c2ae5f4b8310"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
